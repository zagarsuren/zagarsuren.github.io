# Reframing and Communicating AI Ethics

Artificial intelligence (AI) significantly impacts diverse domains such as science and technology, industry, and even our everyday lives. These technologies have been created to analyse data, extract valuable insights, classify, predict, and facilitate innovative decision-making. This is expected to foster the development of new applications and the continued expansion of AI systems. However, these systems have prompted essential inquiries regarding appropriate actions for us, the desired functions of the systems, the potential risks they pose, and strategies for exerting control over them. Several issues arise amid the proliferation of AI systems, such as biased decisions, privacy and data security issues, surveillance, behaviour manipulation, power imbalance and many more. Minimising the risks associated with AI is crucial; therefore, developing and conceptualising AI ethics, finding potential blind spots of AI ethics and reframing it greatly impact society. The questions arise, “What is AI ethics, and how do we conceptualise AI ethics?” “What frameworks and principles should we follow in AI Ethics? “What are the current frameworks, and are they good enough?” “How can communities benefit from AI development and not be discriminated by AI systems?” “How can we regulate and avoid power imbalance associated with AI?”

**Ethics theory and its application to AI**

Critics who take a radical stance on the current technological landscape raise significant questions about the feasibility of conceptualising AI ethics independently of the historical influences and current expressions of capitalism, colonialism, patriarchy, and racism. Even with the potential for widespread application, AI ethics’ theory and implementation face criticism for their perceived deficiency in philosophical underpinnings. This suggests concerns about the theoretical basis and practical application of ethical considerations in the field of artificial intelligence (Bakiner, 2023).

At its fundamental core, ethics involves distinguishing between good and bad and right and wrong. Ethics involves a structured contemplation of what is considered moral. Morality encompasses the entirety of individual or collective opinions, decisions, and actions through which people express their beliefs about what is deemed good or right (Van De Poel & Royakkers, 2011).

Ethical theories seek to address the question of what distinguishes the ethical merit of an action from that of an alternative. Fundamental ethical theories, such as consequentialism and deontology, exemplify attempts to provide insights into this question (Stahl, 2021). 
Consequentialist theories centre on assessing actions based on their outcomes. The outcome is calculating the total utility and disutility of a specific course of action. The ethically optimal choice is identified as the one with the greatest net utility, which is the difference between utility and disutility.
In deontology theories, the agent executing the action is responsible for the ethical evaluation. The theory can be readily employed in various practical scenarios by extending moral obligations to all individuals in every circumstance. Emphasising human motives places ethics squarely within our influence — we might not always manage or foresee the outcomes of our actions, but we retain absolute control over our intentions. Utilising ethical theories in specific domains has given rise to extensive discussions on topics like computer ethics and technology ethics, all related to the field of AI. 

The other definition explains ethics as a descriptive and normative side. Descriptive ethics outlines the current state of morality, encompassing customs, habits, views on good and evil, responsible and irresponsible behaviour, and acceptable and unacceptable actions. On the other hand, normative ethics assesses morality and endeavours to create normative guidelines regarding how one should act or live (Van De Poel & Royakkers, 2011). 

Inherent to its essence, computing technology carries a normative aspect. We anticipate that programs will work for specific goals, such as accurately calculating the specific tasks. The designated purpose acts as a standard for assessment; in other words, we evaluate the computer program’s effectiveness in computing tasks. Considering computers as technological agents is sensible because they perform tasks on our behalf. They function as normative agents to a certain extent, as we can gauge their performance based on how effectively they fulfil their designated tasks (Moor, 2006). Moor mentioned the importance of machine ethics: “We want machines to treat us well, and as machines are becoming more sophisticated, more powerful machines need more powerful machine ethics, programming and teaching a machine to act ethically will help us to understand ethics better”. 

**Ethical frameworks and their issues**

Several countries, such as the US, Canada, United Kingdom, Australia, France, Germany, India, China, and Singapore, have developed their ethical principles and guidelines. However, there are some drawbacks to AI ethics principles and frameworks.

In 2019, the European Commission’s High-level expert group on AI developed the Ethics Guidelines for Trustworthy AI (European Commission & Directorate-General for Communications Networks, 2019). The guidelines constituted a component of the European Union’s endeavours to create a structure for creating and implementing artificial intelligence in accordance with ethical principles. However, the guidelines are broad and open to interpretation, posing a potential challenge for organisations aiming to implement them consistently. Due to the diverse cultural and ethical perspectives across various regions and countries, establishing universal ethical standards for AI development and deployment is challenging, given the global nature of the field.

The Australian AI ethics framework, a discussion paper, was published in the same year (Dave Dawson et al., 2019). It comprises eight core principles: Generates net benefits, Human-centred values, Regulatory and legal compliance, Privacy protection, Fairness, Transparency and explainability, Contestability, and Accountability. 

| Principles | Principles |
|--------------|----------------------|
|**1. Generates net-benefits.** The AI system must generate benefits for people that are greater than the costs.| **2. Do no harm.** Civilian AI systems must not be designed to harm or deceive people and should be implemented in ways that minimise any negative outcomes. |
|**3. Regulatory and legal compliance.** The AI system must comply with all relevant international, Australian Local, State/Territory and Federal government obligations, regulations and laws.| **4. Privacy protection.** Any system, including AI systems, must ensure people’s private data is protected and kept confidential plus prevent data breaches which could cause reputational, psychological, financial, professional or other types of harm.| 
|**5. Fairness.** The development or use of the AI system must not result in unfair discrimination against individuals, communities or groups. This requires particular attention to ensure the “training data” is free from bias or characteristics which may cause the algorithm to behave unfairly. | **6. Transparency & Explainability.** People must be informed when an algorithm is being used that impacts them and they should be provided with information about what information the algorithm uses to make decisions.|
|**7. Contestability.** When an algorithm impacts a person there must be an efficient process to allow that person to challenge the use or output of the algorithm.| **8. Accountability.** People and organisations responsible for the creation and implementation of AI algorithms should be identifiable and accountable for the impacts of that algorithm, even if the impacts are unintended. |

*Core principles for AI (Dave Dawson et al., 2019)*

However, there are some drawbacks to these ethical frameworks. The University of Melbourne researchers reviewed and suggested future developments in the discussion paper (Chris et al., 2019). Firstly, ethical concerns extend beyond AI and encompass a broader range of digital technologies. Privacy, fairness, and harm prevention are ethical challenges that emerge across various technological domains. It is critical to approach ethical considerations holistically, acknowledging that many technologies share similar ethical concerns. It is potential for organisations to evade ethical responsibilities by categorising their applications as something other than AI. This raises the issue of accountability and the need for comprehensive guidelines that cover a broad spectrum of technologies. Suppose specific ethical guidelines are narrowly tailored to AI. In that case, there is a risk that organisations might attempt to reclassify their technologies to escape the associated responsibilities. A more inclusive approach to ethical guidelines helps ensure that organisations are held accountable for the ethical implications of their technological innovations, regardless of the specific terminology used.
Secondly, there are flaws in privacy-related issues. De-identification involves removing or modifying personally identifiable information from datasets to protect privacy. However, advances in data analytics and the potential for re-identification attacks mean that even seemingly anonymised data can sometimes be traced back to individuals. 
Thirdly, regardless of de-identification efforts, obtaining explicit consent from data owners before sharing their information remains a fundamental ethical and legal requirement. Consent ensures that individuals know how their data will be used and can decide whether to share it. However, obtaining informed consent becomes challenging in situations with a significant power imbalance, such as when an individual must use a specific application for work. The individual may feel pressured to consent without fully understanding the implications of data usage.
Finally, the framework needs to include the inclusion of the Indigenous Australians’ involvement in developing these technologies. An ethics framework should prioritise and actively promote the inclusion of Indigenous voices in developing AI technologies. This involves ensuring that Indigenous persons are not just subjects of technology but active participants in shaping its agenda. This inclusion should extend to all stages of technology development, from conception and design to implementation and evaluation. Acknowledging and respecting Indigenous data sovereignty is a critical aspect of ethical considerations. Indigenous communities have unique cultural, social, and historical relationships with their data. An ethics framework should recognise and uphold the right of Indigenous communities to control, manage, and govern their data in a manner that aligns with their values and priorities (Indigenous Data Sovereignty, 2016).

Many AI ethics frameworks need more precise enforcement mechanisms. Even with well-defined ethical principles, there may be a need for more accountability and consequences for non-compliance. Addressing these drawbacks requires ongoing collaboration among stakeholders, continuous refinement of frameworks, and a commitment to adapt ethical guidelines to the evolving landscape of AI technology.

**Power imbalance**

There are other in-focus and out-of-focus blind spots in the AI ethics (Hagendorff, 2022). In-focus issues include calculable issues, such as explainability of AI systems, fairness, and privacy-related issues. On the other hand, out-of-focus issues or negative externalities are hardly or not discussed in AI ethics at all. From postcolonial perspectives, negative externalities emphasise the enduring impact of historical colonialism on contemporary power structures. The deployment of AI technologies in countries with fragile democracies or restricted access to human rights protection can amplify existing power imbalances and potentially lead to the abuse of technology for control or surveillance (Milan & Treré, 2019). Jobin et al. (2019) identified and analysed 84 eligible documents containing ethical principles for AI in different geographical regions. The fact that certain geographical areas, such as Africa, South and Central America, and Central Asia, are not adequately represented in the AI ethics debate suggests an unequal participation of global regions. This lack of equal representation highlights a power imbalance in the international discourse on AI (Jobin et al., 2019). 

The development and deployment of advanced technologies like AI can be seen as a continuation of power dynamics, where the centres of technological innovation are often in economically developed countries (Mohamed et al., 2020). Advanced technologies, including AI, may contribute to economic disparities by concentrating benefits in technologically advanced regions. This concentration of wealth and innovation in certain areas can exacerbate existing global economic inequalities, creating negative externalities for regions with limited access to technological resources or opportunities.

Ricaurte’s theoretical model delves into the coloniality of technological power with a specific focus on data-centric epistemologies. Coloniality often refers to colonial structures’ enduring legacy and impact on contemporary societies. In the context of technological power, it implies that the development and deployment of technology may perpetuate colonial dynamics, reinforcing existing power imbalances (Ricaurte, 2019).

According to the OECD’s statistics, the US and China are the top two countries regarding investment in AI. By 2023, the US is expected to invest 68 billion USD, while China will reach 15 billion USD in AI investment (OECD.AI, 2023). The development and deployment of advanced AI systems require significant resources, including financial, technical, and human capital. Countries or organisations with more significant resources, such as the US and China, may disproportionately influence AI development, leading to global power imbalances. In addition, the widespread adoption of AI-driven automation could lead to job displacement, affecting specific industries and communities more than others. Power imbalances may emerge if certain groups experience economic hardships while others benefit disproportionately from the efficiency gains. Only some have equal access to AI technologies, whether due to economic disparities, digital divides, or lack of infrastructure. This unequal access can widen existing gaps, limiting education, employment, and societal participation opportunities.


In a recent empirical study, 221 peer-reviewed AI ethics articles were analysed (Bakiner, 2023). In this study, the prominence of legislation as the most discussed solution to mitigate AI’s negative social impact aligns with the growing recognition of the need for legal frameworks to govern the development and deployment of AI technologies. As AI continues to evolve, ongoing discussions about responsible development and ethical considerations, as highlighted in the empirical studies, will be crucial in shaping the impact of AI on society. Addressing the risks requires a concerted effort from policymakers, technologists, ethicists, and the community. In addition to current AI ethical principles, I believe the inclusion, data sovereignty, and more precise enforcement mechanisms to follow the principles should be included. Striking a balance between innovation and ethical considerations is crucial to ensure that AI benefits humanity rather than worsening existing power imbalances and risks. 

---

### References

Bakiner, O. (2023). What do academics say about artificial intelligence ethics? An overview of the scholarship. AI and Ethics, 3(2), 513–525. https://doi.org/10.1007/s43681-022-00182-4

Chris, C., Mark, F., Assyl, H., Mark, H., John, H., Reeva, L., Tim, M., Scott, M., Jeannie, P., Megan, R., Ben, R., Liz, S., Mark, T., Vanessa, T., & Monica, W. (2019). Artificial Intelligence: Australia’s Ethics Framework. University of Melbourne Response. https://about.unimelb.edu.au/__data/assets/pdf_file/0031/99184/UoM_response_Data61_AI_Ethics_Framework.pdf

Dave Dawson, Emma Schleiger, Joanna Horton, & et al. (2019). Artificial Intelligence: Australia’s ethics framework—A discussion paper. https://apo.org.au/node/229596

European Commission, & Directorate-General for Communications Networks, C. and T. (2019). Ethics guidelines for trustworthy AI. Publications Office. https://doi.org/10.2759/346720

Hagendorff, T. (2022). Blind spots in AI ethics. AI and Ethics, 2(4), 851–867. https://doi.org/10.1007/s43681-021-00122-8
Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389–399. https://doi.org/10.1038/s42256-019-0088-2

Kukutai, T., & Taylor, J. (Eds.). (2016). Indigenous Data Sovereignty (Vol. 38). ANU Press; JSTOR. http://www.jstor.org/stable/j.ctt1q1crgf

Milan, S., & Treré, E. (2019). Big Data from the South(s): Beyond Data Universalism. Television & New Media, 20(4), 319–335. https://doi.org/10.1177/1527476419837739

Mohamed, S., Png, M.-T., & Isaac, W. (2020). Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy & Technology, 33(4), 659–684. https://doi.org/10.1007/s13347-020-00405-8

Moor, J. H. (2006). The Nature, Importance, and Difficulty of Machine Ethics. IEEE Intelligent Systems, 21(4), 18–21. https://doi.org/10.1109/MIS.2006.80

OECD.AI (2023), Visualisations powered by JSI using data from Preqin, accessed on 13/11/2023, www.oecd.ai .

Ricaurte, P. (2019). Data Epistemologies, The Coloniality of Power, and Resistance. Television & New Media, 20(4), 350–365. https://doi.org/10.1177/1527476419831640

Stahl, B. C. (2021). Concepts of Ethics and Their Application to AI. In B. C. Stahl, Artificial Intelligence for a Better Future (pp. 19–33). Springer International Publishing. https://doi.org/10.1007/978-3-030-69978-9_3

Van De Poel, I., & Royakkers, L. (2011). Ethics, Technology, and Engineering. Wiley.